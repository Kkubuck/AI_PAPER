{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "n_epochs = 15\n",
    "img_size = 224\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader, device):\n",
    "    correct_pred = 0\n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "            X, y_true = X.to(device), y_true.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "            _, predicted_labels = torch.max(y_pred, 1)\n",
    "            \n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "    \n",
    "    return correct_pred.float() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for X, y_true in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X, y_true = X.to(device), y_true.to(device)\n",
    "        y_hat = model(X)\n",
    "        loss = criterion(y_hat, y_true)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    for X, y_true in valid_loader:\n",
    "        X, y_true = X.to(device), y_true.to(device)\n",
    "        y_hat = model(X)\n",
    "        loss = criterion(y_hat, y_true)\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "    \n",
    "    return model, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every = 1):\n",
    "    best_loss = 1e10\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model, valid_loss = validation(valid_loader, model, criterion, device)\n",
    "            valid_losses.append(valid_loss)\n",
    "        \n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            train_acc = get_accuracy(model, train_loader, device)\n",
    "            valid_acc = get_accuracy(model, valid_loader, device)\n",
    "            \n",
    "            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
    "                  f'Epoch: {epoch}\\t'\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "          \n",
    "    return model, optimizer, (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize(224),\n",
    "                transforms.Normalize([meanR, meanG, meanB],\n",
    "                                     [stdR, stdG, stdB])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar10\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e282febbc9341999bb64f1428e26d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cifar10\\cifar-10-python.tar.gz to cifar10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root = 'cifar10',\n",
    "                                 train = True,\n",
    "                                 transform = common_transforms,\n",
    "                                 download = True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root = 'cifar10',\n",
    "                                train = False,\n",
    "                                transform = common_transforms,\n",
    "                                download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "meanRGB = [np.mean(x.numpy(), axis = (1, 2)) for x, _ in train_dataset]\n",
    "stdRGB = [np.std(x.numpy(), axis = (1, 2)) for x, _ in train_dataset]\n",
    "\n",
    "meanR = np.mean([m[0] for m in meanRGB])\n",
    "meanG = np.mean([m[1] for m in meanRGB])\n",
    "meanB = np.mean([m[2] for m in meanRGB])\n",
    "\n",
    "stdR = np.mean([m[0] for m in stdRGB])\n",
    "stdG = np.mean([m[1] for m in stdRGB])\n",
    "stdB = np.mean([m[2] for m in stdRGB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize(224),\n",
    "                transforms.Normalize([meanR, meanG, meanB],\n",
    "                                     [stdR, stdG, stdB])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 8000 val : 2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 0)\n",
    "indices = list(range(len(test_dataset)))\n",
    "y_test0 = [y for _, y in test_dataset]\n",
    "\n",
    "for test_index, val_index in sss.split(indices, y_test0):\n",
    "    print('test :', len(test_index) , 'val :', len(val_index))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "valid_dataset = Subset(test_dataset, val_index)\n",
    "test_dataset = Subset(test_dataset, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, in_channels = 3, num_classes = 1000):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "\n",
    "        self.conv1 = conv_block(in_channels = in_channels, out_channels = 64,\n",
    "                                kernel_size = (7, 7), stride = (2, 2), padding = (3, 3))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size= 3, stride = 2, padding = 1)\n",
    "\n",
    "        self.conv2 = conv_block(64, 192, kernel_size = 3, padding = 1, stride = 1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception3a = Inception_block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception_block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1)\n",
    "\n",
    "        self.inception4a = Inception_block(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception_block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception_block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception_block(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception_block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1)\n",
    "\n",
    "        self.inception5a = Inception_block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception_block(832, 384, 192, 384, 48, 128, 128)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(7, 1)\n",
    "        self.dropout = nn.Dropout(p = 0.4)\n",
    "        self.fc1 = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Inception_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n",
    "        super(Inception_block, self).__init__()\n",
    "        \n",
    "        self.branch1 = conv_block(in_channels, out_1x1, kernel_size = 1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            conv_block(in_channels, red_3x3, kernel_size = 1),\n",
    "            conv_block(red_3x3, out_3x3, kernel_size = 3, padding = 1)\n",
    "        )\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            conv_block(in_channels, red_5x5, kernel_size = 1),\n",
    "            conv_block(red_5x5, out_5x5, kernel_size = 5,  padding = 2)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride = 1, padding = 1),\n",
    "            conv_block(in_channels, out_1x1pool, kernel_size = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x filters x 28 x 28\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1)\n",
    "\n",
    "\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, **kwargs)\n",
    "                            # kernel_size = (1, 1), (3, 3), (5,5 )\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:20:38 --- Epoch: 0\tTrain loss: 1.4679\tValid loss: 1.1314\tTrain accuracy: 60.79\tValid accuracy: 60.90\n",
      "18:23:11 --- Epoch: 1\tTrain loss: 0.9849\tValid loss: 0.8733\tTrain accuracy: 72.40\tValid accuracy: 68.80\n",
      "18:25:41 --- Epoch: 2\tTrain loss: 0.7544\tValid loss: 0.7010\tTrain accuracy: 78.85\tValid accuracy: 75.85\n",
      "18:28:11 --- Epoch: 3\tTrain loss: 0.6214\tValid loss: 0.6285\tTrain accuracy: 83.98\tValid accuracy: 77.75\n",
      "18:30:40 --- Epoch: 4\tTrain loss: 0.5217\tValid loss: 0.5568\tTrain accuracy: 86.72\tValid accuracy: 80.35\n",
      "18:33:10 --- Epoch: 5\tTrain loss: 0.4428\tValid loss: 0.5741\tTrain accuracy: 88.11\tValid accuracy: 80.95\n",
      "18:35:41 --- Epoch: 6\tTrain loss: 0.3777\tValid loss: 0.5383\tTrain accuracy: 90.89\tValid accuracy: 82.75\n",
      "18:38:07 --- Epoch: 7\tTrain loss: 0.3287\tValid loss: 0.6334\tTrain accuracy: 89.15\tValid accuracy: 80.15\n",
      "18:40:34 --- Epoch: 8\tTrain loss: 0.2717\tValid loss: 0.5664\tTrain accuracy: 92.49\tValid accuracy: 81.70\n",
      "18:43:01 --- Epoch: 9\tTrain loss: 0.2313\tValid loss: 0.4995\tTrain accuracy: 94.99\tValid accuracy: 84.60\n",
      "18:45:28 --- Epoch: 10\tTrain loss: 0.1913\tValid loss: 0.5432\tTrain accuracy: 95.00\tValid accuracy: 83.80\n",
      "18:47:54 --- Epoch: 11\tTrain loss: 0.1605\tValid loss: 0.5186\tTrain accuracy: 96.77\tValid accuracy: 84.50\n",
      "18:50:21 --- Epoch: 12\tTrain loss: 0.1449\tValid loss: 0.7802\tTrain accuracy: 94.64\tValid accuracy: 81.65\n",
      "18:52:48 --- Epoch: 13\tTrain loss: 0.1244\tValid loss: 0.7143\tTrain accuracy: 96.46\tValid accuracy: 81.40\n",
      "18:55:15 --- Epoch: 14\tTrain loss: 0.1103\tValid loss: 0.6272\tTrain accuracy: 97.31\tValid accuracy: 84.45\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GoogLeNet(num_classes= 10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, valid_loader, 15, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
