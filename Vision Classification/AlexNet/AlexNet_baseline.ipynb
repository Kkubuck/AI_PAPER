{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled20.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "p5vOgvculHfm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 42\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "n_epochs = 15\n",
        "\n",
        "img_size = 224\n",
        "n_classes = 10"
      ],
      "metadata": {
        "id": "1ZdQ5-qLlHXy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data_loader, device):\n",
        "  correct_pred = 0\n",
        "  n = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for X, y_true in data_loader:\n",
        "      X, y_true = X.to(device), y_true.to(device)\n",
        "\n",
        "      y_pred = model(X)\n",
        "      _, predicted_labels = torch.max(y_pred, 1)\n",
        "\n",
        "      n += y_true.size(0)\n",
        "      correct_pred += (predicted_labels == y_true).sum()\n",
        "  \n",
        "  return correct_pred.float() / n"
      ],
      "metadata": {
        "id": "yW_6UTgFlHVM"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "  model.train()\n",
        "  running_loss = 0\n",
        "  for X, y_true in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    X, y_true = X.to(device), y_true.to(device)\n",
        "    y_hat = model(X)\n",
        "    loss = criterion(y_hat, y_true)\n",
        "    running_loss += loss.item() * X.size(0)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  epoch_loss = running_loss / len(train_loader.dataset)\n",
        "  return model, optimizer, epoch_loss"
      ],
      "metadata": {
        "id": "J1vPQbNElHN6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(valid_loader, model, criterion, device):\n",
        "  model.eval()\n",
        "  running_loss = 0\n",
        "  for X, y_true in valid_loader:\n",
        "    X, y_true = X.to(device), y_true.to(device)\n",
        "\n",
        "    y_hat= model(X)\n",
        "    loss = criterion(y_hat, y_true)\n",
        "    running_loss += loss.item() * X.size(0)\n",
        "  \n",
        "  epoch_loss = running_loss / len(valid_loader.dataset)\n",
        "\n",
        "  return model, epoch_loss"
      ],
      "metadata": {
        "id": "99_Sj8Ugn0zr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every = 1):\n",
        "  best_loss = 1e10\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model, valid_loss = validation(valid_loader, model, criterion, device)\n",
        "      valid_losses.append(valid_loss)\n",
        "\n",
        "    if epoch % print_every == (print_every - 1):\n",
        "      train_acc = get_accuracy(model, train_loader, device)\n",
        "      valid_acc = get_accuracy(model, valid_loader, device)\n",
        "            \n",
        "      print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
        "            f'Epoch: {epoch}\\t'\n",
        "            f'Train loss: {train_loss:.4f}\\t'\n",
        "            f'Valid loss: {valid_loss:.4f}\\t'\n",
        "            f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
        "            f'Valid accuracy: {100 * valid_acc:.2f}')\n",
        "          \n",
        "  return model, optimizer, (train_losses, valid_losses)      "
      ],
      "metadata": {
        "id": "NozUUWJuoMYs"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_transforms = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Resize(227)\n",
        "])"
      ],
      "metadata": {
        "id": "JZsJfberq8P4"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root = 'cifar10',\n",
        "                                 train = True,\n",
        "                                 transform = common_transforms,\n",
        "                                 download = True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root = 'cifar10',\n",
        "                                train = False,\n",
        "                                transform = common_transforms,\n",
        "                                download = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvhVHD4WqQFk",
        "outputId": "18398648-80a7-4aad-8fd6-aac550b34c61"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "meanRGB = [np.mean(x.numpy(), axis = (1, 2)) for x, _ in train_dataset]\n",
        "stdRGB = [np.std(x.numpy(), axis = (1, 2)) for x, _ in train_dataset]\n",
        "\n",
        "meanR = np.mean([m[0] for m in meanRGB])\n",
        "meanG = np.mean([m[1] for m in meanRGB])\n",
        "meanB = np.mean([m[2] for m in meanRGB])\n",
        "\n",
        "stdR = np.mean([s[0] for s in stdRGB])\n",
        "stdG = np.mean([s[1] for s in stdRGB])\n",
        "stdB = np.mean([s[2] for s in stdRGB])\n",
        "\n",
        "print(meanR, meanG, meanB)\n",
        "print(stdR, stdG, stdB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxuLQqInq7wH",
        "outputId": "9eab6b61-217c-467a-fa56-ee6cb0727c8c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.49140054 0.4821596 0.4465322\n",
            "0.19510235 0.19232473 0.19405492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transformer = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Resize((227, 227)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.Normalize([0.49139965, 0.48215845, 0.4465309],\n",
        "                                     [0.20220213, 0.19931543, 0.20086348])\n",
        "])\n",
        "\n",
        "test_transformer = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Resize((227, 227)),\n",
        "                transforms.Normalize([0.49139965, 0.48215845, 0.4465309],\n",
        "                                     [0.20220213, 0.19931543, 0.20086348])\n",
        "])"
      ],
      "metadata": {
        "id": "QxrUliAMpP8I"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.transforms = train_transformer\n",
        "test_dataset.transforms = test_transformer"
      ],
      "metadata": {
        "id": "wFzdmaMpsYVm"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 0)\n",
        "indices = list(range(len(test_dataset)))\n",
        "y_test0 = [y for _, y in test_dataset]\n",
        "\n",
        "for test_index, val_index in sss.split(indices, y_test0):\n",
        "    print('test :', len(test_index) , 'val :', len(val_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ8F3xQ8shTC",
        "outputId": "9b1e6ac9-ceaa-4ea2-dcc5-fa846db8405b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test : 8000 val : 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "valid_dataset = Subset(test_dataset, val_index)\n",
        "test_dataset = Subset(test_dataset, test_index)"
      ],
      "metadata": {
        "id": "T6nEGHqFtD-T"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "WK-5ukMet0UJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "HzlOw4nSFipo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "    super(AlexNet, self).__init__()\n",
        "    \n",
        "    self.feature_extractor = nn.Sequential(\n",
        "      # First Layer\n",
        "      nn.Conv2d(in_channels = 3, out_channels = 96, kernel_size = 11, stride = 4),\n",
        "      nn.ReLU(inplace = True),\n",
        "      nn.LocalResponseNorm(size = 5, alpha = 1e-3, beta = 0.75, k = 2),\n",
        "      nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "\n",
        "      # Second Layer\n",
        "      nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size = 5,stride = 1, padding = 2),\n",
        "      nn.ReLU(),\n",
        "      nn.LocalResponseNorm(size = 5, alpha = 1e-4, beta = 0.75, k = 2),\n",
        "      nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "\n",
        "      # Third Layer\n",
        "      nn.Conv2d(in_channels = 256, out_channels = 384, kernel_size = 3, stride = 1, padding = 1),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      # Fourth Layer\n",
        "      nn.Conv2d(in_channels = 384, out_channels = 384, kernel_size = 3, stride = 1, padding = 1),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Conv2d(in_channels = 384, out_channels = 256, kernel_size = 3, stride = 1, padding = 1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(3, 2),\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(256 * 6 * 6, out_features = 4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(in_features = 4096, out_features = 4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features = 4096, out_features = n_classes),\n",
        "    )\n",
        "\n",
        "  def init_weight(self):\n",
        "    for layer in self.feature_extractor:\n",
        "      if isinstance(layer, nn.Conv2d):\n",
        "        nn.init.normal_(layer.weight, mean = 0, std = 0.01)\n",
        "        nn.init.constant_(layer.bias, 0)\n",
        "    nn.init.constant_(self.net[4].bias, 1)\n",
        "    nn.init.constant_(self.net[10].bias, 1)\n",
        "    nn.init.constant_(self.net[12].bias, 1)\n",
        "  \n",
        "  def forward(self , x):\n",
        "    x = self.feature_extractor(x)\n",
        "    x = x.view(-1, 256*6*6)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "3Zj3ceJxuclc"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet(10).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, valid_loader, 15, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuDG9ajJJwq8",
        "outputId": "7c3e4bc7-f7e2-4d79-8977-e39eb0010dc5"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12:23:04 --- Epoch: 0\tTrain loss: 1.8347\tValid loss: 1.5417\tTrain accuracy: 42.92\tValid accuracy: 42.70\n",
            "12:25:24 --- Epoch: 1\tTrain loss: 1.4907\tValid loss: 1.3693\tTrain accuracy: 51.39\tValid accuracy: 50.70\n",
            "12:27:44 --- Epoch: 2\tTrain loss: 1.3357\tValid loss: 1.2783\tTrain accuracy: 57.36\tValid accuracy: 54.50\n",
            "12:30:06 --- Epoch: 3\tTrain loss: 1.2157\tValid loss: 1.1371\tTrain accuracy: 63.58\tValid accuracy: 60.25\n",
            "12:32:27 --- Epoch: 4\tTrain loss: 1.1159\tValid loss: 1.0581\tTrain accuracy: 67.13\tValid accuracy: 63.30\n",
            "12:34:47 --- Epoch: 5\tTrain loss: 1.0466\tValid loss: 0.9996\tTrain accuracy: 70.32\tValid accuracy: 64.75\n",
            "12:37:07 --- Epoch: 6\tTrain loss: 0.9750\tValid loss: 1.0412\tTrain accuracy: 69.75\tValid accuracy: 63.20\n",
            "12:39:29 --- Epoch: 7\tTrain loss: 0.9231\tValid loss: 0.9490\tTrain accuracy: 74.08\tValid accuracy: 66.90\n",
            "12:41:50 --- Epoch: 8\tTrain loss: 0.8834\tValid loss: 0.9235\tTrain accuracy: 76.13\tValid accuracy: 68.50\n",
            "12:44:10 --- Epoch: 9\tTrain loss: 0.8471\tValid loss: 0.8698\tTrain accuracy: 78.31\tValid accuracy: 69.80\n",
            "12:46:30 --- Epoch: 10\tTrain loss: 0.8156\tValid loss: 0.9030\tTrain accuracy: 78.05\tValid accuracy: 69.80\n",
            "12:48:51 --- Epoch: 11\tTrain loss: 0.7858\tValid loss: 0.9309\tTrain accuracy: 76.67\tValid accuracy: 67.65\n",
            "12:51:12 --- Epoch: 12\tTrain loss: 0.7655\tValid loss: 0.9165\tTrain accuracy: 79.66\tValid accuracy: 69.65\n",
            "12:53:32 --- Epoch: 13\tTrain loss: 0.7446\tValid loss: 0.8579\tTrain accuracy: 80.92\tValid accuracy: 70.95\n",
            "12:55:53 --- Epoch: 14\tTrain loss: 0.7137\tValid loss: 0.8371\tTrain accuracy: 82.76\tValid accuracy: 70.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHUlkbPnwGam",
        "outputId": "7f27f49b-c0e4-4168-d583-401fa809596d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "VyooSJuMwII8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size = (3, 227, 227))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQRS8NCRwKYg",
        "outputId": "f6e8a349-7fc7-456c-ca56-732544ab35c2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
            "              ReLU-2           [-1, 96, 55, 55]               0\n",
            " LocalResponseNorm-3           [-1, 96, 55, 55]               0\n",
            "         MaxPool2d-4           [-1, 96, 27, 27]               0\n",
            "            Conv2d-5          [-1, 256, 27, 27]         614,656\n",
            "              ReLU-6          [-1, 256, 27, 27]               0\n",
            " LocalResponseNorm-7          [-1, 256, 27, 27]               0\n",
            "         MaxPool2d-8          [-1, 256, 13, 13]               0\n",
            "            Conv2d-9          [-1, 384, 13, 13]         885,120\n",
            "             ReLU-10          [-1, 384, 13, 13]               0\n",
            "           Conv2d-11          [-1, 384, 13, 13]       1,327,488\n",
            "             ReLU-12          [-1, 384, 13, 13]               0\n",
            "           Conv2d-13          [-1, 256, 13, 13]         884,992\n",
            "             ReLU-14          [-1, 256, 13, 13]               0\n",
            "        MaxPool2d-15            [-1, 256, 6, 6]               0\n",
            "          Dropout-16                 [-1, 9216]               0\n",
            "           Linear-17                 [-1, 4096]      37,752,832\n",
            "             ReLU-18                 [-1, 4096]               0\n",
            "          Dropout-19                 [-1, 4096]               0\n",
            "           Linear-20                 [-1, 4096]      16,781,312\n",
            "             ReLU-21                 [-1, 4096]               0\n",
            "           Linear-22                   [-1, 10]          40,970\n",
            "================================================================\n",
            "Total params: 58,322,314\n",
            "Trainable params: 58,322,314\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.59\n",
            "Forward/backward pass size (MB): 14.72\n",
            "Params size (MB): 222.48\n",
            "Estimated Total Size (MB): 237.79\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7w05rFqTwPUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}