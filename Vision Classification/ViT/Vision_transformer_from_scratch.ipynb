{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbyjCv-sazUw",
        "outputId": "b9bd0210-440b-406e-814b-f0c7fc88af06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EqP2MGf2YgEz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 3, patch_size = 16, \n",
        "                 emb_size = 768, img_size = 224):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # 2가지 방식이 존재합니다.\n",
        "        # Method 1 : Flatten and FC Layer\n",
        "        # self.projection = nn.Sequential(\n",
        "        #     Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1 = patch_size, s2 = patch_size),\n",
        "        #     nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        # )\n",
        "\n",
        "        # Method 2: Conv\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, emb_size, patch_size, stride = patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e')\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, emb_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b = x.shape[0]\n",
        "        x = self.projection(x)\n",
        "        # print('1', x.size())\n",
        "        cls_token = repeat(self.cls_token, '() n e -> b n e', b = b)\n",
        "        # print('2', cls_token.size())\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_token, x], dim = 1)\n",
        "        # print('3', x.size())\n",
        "        x += self.positions\n",
        "        # print('4', x.size())\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_07xfiu6Ys6B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check PatchEmbedding\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "x = torch.randn(16, 3 , 224, 224).to(device)\n",
        "patch_embedding = PatchEmbedding().to(device)\n",
        "patch_output = patch_embedding(x)\n",
        "patch_output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrSErZ8td9tS",
        "outputId": "7bc889a8-c958-48b7-d3de-8b8feb82bb09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size = 768, num_heads = 8, dropout = 0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "    \n",
        "    def forward(self, x, mask = None):\n",
        "        queries = rearrange(self.queries(x), 'b n (h d) -> b h n d', h = self.num_heads)\n",
        "        keys = rearrange(self.keys(x), 'b n (h d) -> b h n d', h = self.num_heads)\n",
        "        values = rearrange(self.values(x), 'b n (h d) -> b h n d', h = self.num_heads)\n",
        "\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "        \n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        att = F.softmax(energy, dim = -1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav', att, values)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.projection(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zoR89lnGeNhj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MHA = MultiHeadAttention().to(device)\n",
        "MHA_output = MHA(patch_output)\n",
        "print(MHA_output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_dUzrqHizyv",
        "outputId": "1f25c91b-625f-428e-930f-ca35c470873a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x"
      ],
      "metadata": {
        "id": "c9xVInU3i_L9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion=4, drop_p=0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )"
      ],
      "metadata": {
        "id": "LvsyEtfDjTBq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "x = torch.randn(16,1,128).to(device)\n",
        "model = FeedForwardBlock(128).to(device)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jpek_xvjV1z",
        "outputId": "a2932589-caa6-4378-e879-e9eb73d64149"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size=768, drop_p=0., forward_expansion=4, forward_drop_p=0., **kwargs):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(emb_size, expansion = forward_expansion, drop_p = forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            ))\n",
        "        )"
      ],
      "metadata": {
        "id": "eH7U1WfZjXkj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerEncoderBlock().to(device)\n",
        "output = model(patch_output).to(device)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4xKuwvbj1KV",
        "outputId": "79c38d4e-05f7-44f7-f028-0d8584c9de15"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth=12, **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
      ],
      "metadata": {
        "id": "1o2F_Yi0j3Dj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerEncoder().to(device)\n",
        "output = model(patch_output)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK1UFWv2j49c",
        "outputId": "3e614c58-c60c-43bd-e10b-330eb845b6db"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size=768, n_classes = 10):\n",
        "        super().__init__(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, n_classes))"
      ],
      "metadata": {
        "id": "P_xYuAPIj6gA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(16, 1, 768).to(device)\n",
        "model = ClassificationHead().to(device)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jprnjelkB6i",
        "outputId": "e7b45382-455e-44e7-e99e-e0f419f6393b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Sequential):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224, depth=12, n_classes=10, **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )"
      ],
      "metadata": {
        "id": "DKWL0nppkDa4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(16,3,224,224).to(device)\n",
        "model = ViT().to(device)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhKPP8TokFvO",
        "outputId": "ce910fcb-f30b-48f5-bdee-ab6227d10ba5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z862IwHkkG6K",
        "outputId": "c0d4d01d-ce19-4073-9b79-57627b849e5a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG5wFo0Pkpl_",
        "outputId": "4e200140-b129-49b4-b5e5-b90682cca1dd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
            "         Rearrange-2             [-1, 196, 768]               0\n",
            "    PatchEmbedding-3             [-1, 197, 768]               0\n",
            "         LayerNorm-4             [-1, 197, 768]           1,536\n",
            "            Linear-5             [-1, 197, 768]         590,592\n",
            "            Linear-6             [-1, 197, 768]         590,592\n",
            "            Linear-7             [-1, 197, 768]         590,592\n",
            "           Dropout-8          [-1, 8, 197, 197]               0\n",
            "            Linear-9             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-10             [-1, 197, 768]               0\n",
            "          Dropout-11             [-1, 197, 768]               0\n",
            "      ResidualAdd-12             [-1, 197, 768]               0\n",
            "        LayerNorm-13             [-1, 197, 768]           1,536\n",
            "           Linear-14            [-1, 197, 3072]       2,362,368\n",
            "             GELU-15            [-1, 197, 3072]               0\n",
            "          Dropout-16            [-1, 197, 3072]               0\n",
            "           Linear-17             [-1, 197, 768]       2,360,064\n",
            "          Dropout-18             [-1, 197, 768]               0\n",
            "      ResidualAdd-19             [-1, 197, 768]               0\n",
            "        LayerNorm-20             [-1, 197, 768]           1,536\n",
            "           Linear-21             [-1, 197, 768]         590,592\n",
            "           Linear-22             [-1, 197, 768]         590,592\n",
            "           Linear-23             [-1, 197, 768]         590,592\n",
            "          Dropout-24          [-1, 8, 197, 197]               0\n",
            "           Linear-25             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-26             [-1, 197, 768]               0\n",
            "          Dropout-27             [-1, 197, 768]               0\n",
            "      ResidualAdd-28             [-1, 197, 768]               0\n",
            "        LayerNorm-29             [-1, 197, 768]           1,536\n",
            "           Linear-30            [-1, 197, 3072]       2,362,368\n",
            "             GELU-31            [-1, 197, 3072]               0\n",
            "          Dropout-32            [-1, 197, 3072]               0\n",
            "           Linear-33             [-1, 197, 768]       2,360,064\n",
            "          Dropout-34             [-1, 197, 768]               0\n",
            "      ResidualAdd-35             [-1, 197, 768]               0\n",
            "        LayerNorm-36             [-1, 197, 768]           1,536\n",
            "           Linear-37             [-1, 197, 768]         590,592\n",
            "           Linear-38             [-1, 197, 768]         590,592\n",
            "           Linear-39             [-1, 197, 768]         590,592\n",
            "          Dropout-40          [-1, 8, 197, 197]               0\n",
            "           Linear-41             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-42             [-1, 197, 768]               0\n",
            "          Dropout-43             [-1, 197, 768]               0\n",
            "      ResidualAdd-44             [-1, 197, 768]               0\n",
            "        LayerNorm-45             [-1, 197, 768]           1,536\n",
            "           Linear-46            [-1, 197, 3072]       2,362,368\n",
            "             GELU-47            [-1, 197, 3072]               0\n",
            "          Dropout-48            [-1, 197, 3072]               0\n",
            "           Linear-49             [-1, 197, 768]       2,360,064\n",
            "          Dropout-50             [-1, 197, 768]               0\n",
            "      ResidualAdd-51             [-1, 197, 768]               0\n",
            "        LayerNorm-52             [-1, 197, 768]           1,536\n",
            "           Linear-53             [-1, 197, 768]         590,592\n",
            "           Linear-54             [-1, 197, 768]         590,592\n",
            "           Linear-55             [-1, 197, 768]         590,592\n",
            "          Dropout-56          [-1, 8, 197, 197]               0\n",
            "           Linear-57             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-58             [-1, 197, 768]               0\n",
            "          Dropout-59             [-1, 197, 768]               0\n",
            "      ResidualAdd-60             [-1, 197, 768]               0\n",
            "        LayerNorm-61             [-1, 197, 768]           1,536\n",
            "           Linear-62            [-1, 197, 3072]       2,362,368\n",
            "             GELU-63            [-1, 197, 3072]               0\n",
            "          Dropout-64            [-1, 197, 3072]               0\n",
            "           Linear-65             [-1, 197, 768]       2,360,064\n",
            "          Dropout-66             [-1, 197, 768]               0\n",
            "      ResidualAdd-67             [-1, 197, 768]               0\n",
            "        LayerNorm-68             [-1, 197, 768]           1,536\n",
            "           Linear-69             [-1, 197, 768]         590,592\n",
            "           Linear-70             [-1, 197, 768]         590,592\n",
            "           Linear-71             [-1, 197, 768]         590,592\n",
            "          Dropout-72          [-1, 8, 197, 197]               0\n",
            "           Linear-73             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-74             [-1, 197, 768]               0\n",
            "          Dropout-75             [-1, 197, 768]               0\n",
            "      ResidualAdd-76             [-1, 197, 768]               0\n",
            "        LayerNorm-77             [-1, 197, 768]           1,536\n",
            "           Linear-78            [-1, 197, 3072]       2,362,368\n",
            "             GELU-79            [-1, 197, 3072]               0\n",
            "          Dropout-80            [-1, 197, 3072]               0\n",
            "           Linear-81             [-1, 197, 768]       2,360,064\n",
            "          Dropout-82             [-1, 197, 768]               0\n",
            "      ResidualAdd-83             [-1, 197, 768]               0\n",
            "        LayerNorm-84             [-1, 197, 768]           1,536\n",
            "           Linear-85             [-1, 197, 768]         590,592\n",
            "           Linear-86             [-1, 197, 768]         590,592\n",
            "           Linear-87             [-1, 197, 768]         590,592\n",
            "          Dropout-88          [-1, 8, 197, 197]               0\n",
            "           Linear-89             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-90             [-1, 197, 768]               0\n",
            "          Dropout-91             [-1, 197, 768]               0\n",
            "      ResidualAdd-92             [-1, 197, 768]               0\n",
            "        LayerNorm-93             [-1, 197, 768]           1,536\n",
            "           Linear-94            [-1, 197, 3072]       2,362,368\n",
            "             GELU-95            [-1, 197, 3072]               0\n",
            "          Dropout-96            [-1, 197, 3072]               0\n",
            "           Linear-97             [-1, 197, 768]       2,360,064\n",
            "          Dropout-98             [-1, 197, 768]               0\n",
            "      ResidualAdd-99             [-1, 197, 768]               0\n",
            "       LayerNorm-100             [-1, 197, 768]           1,536\n",
            "          Linear-101             [-1, 197, 768]         590,592\n",
            "          Linear-102             [-1, 197, 768]         590,592\n",
            "          Linear-103             [-1, 197, 768]         590,592\n",
            "         Dropout-104          [-1, 8, 197, 197]               0\n",
            "          Linear-105             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-106             [-1, 197, 768]               0\n",
            "         Dropout-107             [-1, 197, 768]               0\n",
            "     ResidualAdd-108             [-1, 197, 768]               0\n",
            "       LayerNorm-109             [-1, 197, 768]           1,536\n",
            "          Linear-110            [-1, 197, 3072]       2,362,368\n",
            "            GELU-111            [-1, 197, 3072]               0\n",
            "         Dropout-112            [-1, 197, 3072]               0\n",
            "          Linear-113             [-1, 197, 768]       2,360,064\n",
            "         Dropout-114             [-1, 197, 768]               0\n",
            "     ResidualAdd-115             [-1, 197, 768]               0\n",
            "       LayerNorm-116             [-1, 197, 768]           1,536\n",
            "          Linear-117             [-1, 197, 768]         590,592\n",
            "          Linear-118             [-1, 197, 768]         590,592\n",
            "          Linear-119             [-1, 197, 768]         590,592\n",
            "         Dropout-120          [-1, 8, 197, 197]               0\n",
            "          Linear-121             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-122             [-1, 197, 768]               0\n",
            "         Dropout-123             [-1, 197, 768]               0\n",
            "     ResidualAdd-124             [-1, 197, 768]               0\n",
            "       LayerNorm-125             [-1, 197, 768]           1,536\n",
            "          Linear-126            [-1, 197, 3072]       2,362,368\n",
            "            GELU-127            [-1, 197, 3072]               0\n",
            "         Dropout-128            [-1, 197, 3072]               0\n",
            "          Linear-129             [-1, 197, 768]       2,360,064\n",
            "         Dropout-130             [-1, 197, 768]               0\n",
            "     ResidualAdd-131             [-1, 197, 768]               0\n",
            "       LayerNorm-132             [-1, 197, 768]           1,536\n",
            "          Linear-133             [-1, 197, 768]         590,592\n",
            "          Linear-134             [-1, 197, 768]         590,592\n",
            "          Linear-135             [-1, 197, 768]         590,592\n",
            "         Dropout-136          [-1, 8, 197, 197]               0\n",
            "          Linear-137             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-138             [-1, 197, 768]               0\n",
            "         Dropout-139             [-1, 197, 768]               0\n",
            "     ResidualAdd-140             [-1, 197, 768]               0\n",
            "       LayerNorm-141             [-1, 197, 768]           1,536\n",
            "          Linear-142            [-1, 197, 3072]       2,362,368\n",
            "            GELU-143            [-1, 197, 3072]               0\n",
            "         Dropout-144            [-1, 197, 3072]               0\n",
            "          Linear-145             [-1, 197, 768]       2,360,064\n",
            "         Dropout-146             [-1, 197, 768]               0\n",
            "     ResidualAdd-147             [-1, 197, 768]               0\n",
            "       LayerNorm-148             [-1, 197, 768]           1,536\n",
            "          Linear-149             [-1, 197, 768]         590,592\n",
            "          Linear-150             [-1, 197, 768]         590,592\n",
            "          Linear-151             [-1, 197, 768]         590,592\n",
            "         Dropout-152          [-1, 8, 197, 197]               0\n",
            "          Linear-153             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-154             [-1, 197, 768]               0\n",
            "         Dropout-155             [-1, 197, 768]               0\n",
            "     ResidualAdd-156             [-1, 197, 768]               0\n",
            "       LayerNorm-157             [-1, 197, 768]           1,536\n",
            "          Linear-158            [-1, 197, 3072]       2,362,368\n",
            "            GELU-159            [-1, 197, 3072]               0\n",
            "         Dropout-160            [-1, 197, 3072]               0\n",
            "          Linear-161             [-1, 197, 768]       2,360,064\n",
            "         Dropout-162             [-1, 197, 768]               0\n",
            "     ResidualAdd-163             [-1, 197, 768]               0\n",
            "       LayerNorm-164             [-1, 197, 768]           1,536\n",
            "          Linear-165             [-1, 197, 768]         590,592\n",
            "          Linear-166             [-1, 197, 768]         590,592\n",
            "          Linear-167             [-1, 197, 768]         590,592\n",
            "         Dropout-168          [-1, 8, 197, 197]               0\n",
            "          Linear-169             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-170             [-1, 197, 768]               0\n",
            "         Dropout-171             [-1, 197, 768]               0\n",
            "     ResidualAdd-172             [-1, 197, 768]               0\n",
            "       LayerNorm-173             [-1, 197, 768]           1,536\n",
            "          Linear-174            [-1, 197, 3072]       2,362,368\n",
            "            GELU-175            [-1, 197, 3072]               0\n",
            "         Dropout-176            [-1, 197, 3072]               0\n",
            "          Linear-177             [-1, 197, 768]       2,360,064\n",
            "         Dropout-178             [-1, 197, 768]               0\n",
            "     ResidualAdd-179             [-1, 197, 768]               0\n",
            "       LayerNorm-180             [-1, 197, 768]           1,536\n",
            "          Linear-181             [-1, 197, 768]         590,592\n",
            "          Linear-182             [-1, 197, 768]         590,592\n",
            "          Linear-183             [-1, 197, 768]         590,592\n",
            "         Dropout-184          [-1, 8, 197, 197]               0\n",
            "          Linear-185             [-1, 197, 768]         590,592\n",
            "MultiHeadAttention-186             [-1, 197, 768]               0\n",
            "         Dropout-187             [-1, 197, 768]               0\n",
            "     ResidualAdd-188             [-1, 197, 768]               0\n",
            "       LayerNorm-189             [-1, 197, 768]           1,536\n",
            "          Linear-190            [-1, 197, 3072]       2,362,368\n",
            "            GELU-191            [-1, 197, 3072]               0\n",
            "         Dropout-192            [-1, 197, 3072]               0\n",
            "          Linear-193             [-1, 197, 768]       2,360,064\n",
            "         Dropout-194             [-1, 197, 768]               0\n",
            "     ResidualAdd-195             [-1, 197, 768]               0\n",
            "          Reduce-196                  [-1, 768]               0\n",
            "       LayerNorm-197                  [-1, 768]           1,536\n",
            "          Linear-198                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 85,654,282\n",
            "Trainable params: 85,654,282\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 364.33\n",
            "Params size (MB): 326.75\n",
            "Estimated Total Size (MB): 691.64\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aszDXWd0ku4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}